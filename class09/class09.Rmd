---
title: "class09- machine learning pt1"
author: "Gayatri Mainkar"
date: "2/4/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The main K means function in R is called kmeans(). Let's play with it here.

```{r}
#Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster?
Q. What ‘component’ of your result object details
      - cluster size?
      - cluster assignment/membership?
      - cluster center?
Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
KM <- kmeans(x, centers = 2, nstart = 20)
KM$cluster
KM$size
KM$centers
plot(x)
points(KM$centers, col = "blue")
```

## Hierarchial clustering in R

The main hierarchial clustering function in R is called 'hclust()' An imortant point here is that you have to calculate the distance matrix deom your input data before calling 'hclust()'.

For this we will use the 'dist()' function first.

```{r}
#We will use our x again from above...
d<- dist(x)
hc<- hclust(d)
plot(hc)
abline(h = 6, col = "red", lty = 2)
abline(h = 3, col = "blue")
```

To get cluster membership vector I need to "cut" the tree at a certain height to yield my separate cluster branches.

```{r}
cutree(hc, h = 6)
```

```{r}
gp <- cutree(hc, k = 3)
table(gp)
```
```{r}
hc.complete <- hclust(d, method="complete")
hc.average  <- hclust(d, method="average")
hc.single   <- hclust(d, method="single")
```

```{r}
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
Q. How does this compare to your known 'col' groups?

```{r}
hc2 <-hclust(dist(x))
plot(hc2)
cutree(hc2, k = 2)
grps <-  cutree(hc2, k = 3)
abline(h = 2.5, col = "blue")
abline(h = 1.8,col = "red") 
plot(x, col = grps)
```

Hands on Section
```{r}
x <- read.csv("UK_foods.csv", row.names = 1)
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
pairs(x, col=rainbow(10), pch=16)
```

PCA with the 'prcomp()' function
```{r}
pca <- prcomp(t(x))
summary(pca)
```

What is in my result object 'pca'? I can check the attributes...
```{r}
plot(pca$x[,1], pca$x[,2])
text(pca$x[,1], pca$x[,2], colnames(x), col = c("black", "red", "blue", "darkgreen"))
```
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

